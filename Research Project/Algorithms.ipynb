{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e0e018-c21f-40b6-acfb-ec57dcb6fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_object_dtype\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69fba52b-9535-4656-8e13-295b7f940213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Datasets/bank-customer-churn-prediction.csv')\n",
    "# df = pd.read_csv('Datasets/financial-risk-for-loan-approval.csv')\n",
    "# df = pd.read_csv('Datasets/loan-approval-classification-dataset.csv')\n",
    "\n",
    "df = pd.read_csv('Datasets/campus-placement-prediction.csv')\n",
    "# df = pd.read_csv('Datasets/predict-dropout-or-academic-success.csv')\n",
    "# df = pd.read_csv('Datasets/student-performance-dataset.csv')\n",
    "\n",
    "# df = pd.read_csv('Datasets/fetal-health-classifiation.csv')\n",
    "# df = pd.read_csv('Datasets/heart-disease-health-indicators-dataset.csv')\n",
    "# df = pd.read_csv('Datasets/patient-treatment-classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6bdfe37-7e73-4e03-b2c5-295f5db63a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(dataframe):\n",
    "    preprocessing(dataframe)\n",
    "    correlation_matrix = dataframe.corr()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f713a932-4506-4f56-b632-0aa3c822be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, eps=1.0, minPts=5, equation='euclidean'):\n",
    "        self.eps = eps # Radius for neighbourhood search\n",
    "        self.minPts = minPts # Minimum number of points to form a cluster\n",
    "        self.equation = equation\n",
    "        \n",
    "    def fit(self, dataframe):\n",
    "        # Initialise labels array with -1\n",
    "        self.labels = np.zeros(len(dataframe), dtype=int) - 1\n",
    "        self.clusters = [] # List to hold all clusters\n",
    "        cidx = 0 # Cluster ID counter\n",
    "        for x in range(len(dataframe)):\n",
    "            if self.labels[x] == -1:\n",
    "                neighbours = self.get_neighbours(dataframe, x)\n",
    "                if len(neighbours) < self.minPts:\n",
    "                    self.labels[x] = 0 # Mark point as anomaly\n",
    "                else:\n",
    "                    cidx += 1\n",
    "                    self.clusters.append([x])\n",
    "                    self.labels[x] = cidx # Assign cluster ID to the point\n",
    "\n",
    "                    # Exapnd the cluster with points\n",
    "                    for y in neighbours:\n",
    "                        if self.labels[y] == -1:\n",
    "                            self.labels[y] = cidx\n",
    "                            self.clusters[cidx-1].append(y)\n",
    "                            neighbours2 = self.get_neighbours(dataframe, y)\n",
    "                            if len(neighbours2) >= self.minPts:\n",
    "                                neighbours += list(set(neighbours2) - set(neighbours))\n",
    "                        elif self.labels[y] == 0:\n",
    "                            self.labels[y] = cidx\n",
    "                            self.clusters[cidx-1].append(y)\n",
    "        return self.clusters\n",
    "\n",
    "    def get_neighbours(self, dataframe, x):\n",
    "        neighbours = set()\n",
    "        # Iterate over all points to calculate distances\n",
    "        for y in range(len(dataframe)):\n",
    "            if self.distance(dataframe[x], dataframe[y]) <= self.eps: # Check if within eps distance\n",
    "                neighbours.add(y)\n",
    "        return list(neighbours)\n",
    "    \n",
    "    def distance(self, x, y):\n",
    "        # Euclidean distance\n",
    "        return np.sqrt(np.sum((x-y)**2))\n",
    "                           \n",
    "\n",
    "    def plot(self, data):\n",
    "        # Create a 2D plot (instead of 3D)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        # Find noise points and clusters\n",
    "        noise_idx = np.where(self.labels == 0)[0]\n",
    "        cluster_idxs = [np.array(c) for c in self.clusters]\n",
    "\n",
    "        # Assign colours to clusters\n",
    "        colours = cm.rainbow(np.linspace(0, 1, len(self.clusters) + 1))\n",
    "\n",
    "        # Plot each cluster\n",
    "        for i, cluster_idx in enumerate(cluster_idxs):\n",
    "            plt.scatter(data[cluster_idx, 0], data[cluster_idx, 1], \n",
    "                        color=colours[i], s=10, label=f'Cluster {i + 1}')\n",
    "\n",
    "        # Plot anomalies\n",
    "        if len(noise_idx) > 0:\n",
    "            plt.scatter(data[noise_idx, 0], data[noise_idx, 1], \n",
    "                        color='black', s=10, label='Anomalies')\n",
    "\n",
    "        # Add labels and legend\n",
    "        plt.title(\"DBSCAN Clustering\")\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc49093a-33fc-4ed9-8cc0-6212c1b8a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCSVM:\n",
    "    def __init__(self, nu=0.1, kernel='rbf', gamma=1.0):\n",
    "        self.nu = nu\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _kernel_function(self, X, Y=None):\n",
    "        # Computer the kernel matrix\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        if self.kernel == 'rbf':\n",
    "            # Radial basis function\n",
    "            # Approximate RBF kernel using Random Fourier Features\n",
    "            n_features = 100  # Dimensionality of approximation\n",
    "            rng = np.random.default_rng(seed=42)\n",
    "            omega = rng.normal(0, np.sqrt(2 * self.gamma), size=(X.shape[1], n_features))\n",
    "            b = rng.uniform(0, 2 * np.pi, size=n_features)\n",
    "            X_proj = np.cos(X @ omega + b)\n",
    "            Y_proj = np.cos(Y @ omega + b) if Y is not X else X_proj\n",
    "            return X_proj @ Y_proj.T\n",
    "        elif self.kernel == 'linear':\n",
    "            # Linear kernel\n",
    "            return np.dot(X, Y.T)\n",
    "        elif self.kernel == 'poly':\n",
    "            # Polynomial kernel\n",
    "            return (np.dot(X, Y.T) + 1) ** 3\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel type\")\n",
    "\n",
    "    def fit(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        K = self._kernel_function(X)\n",
    "\n",
    "        # Define the quadratic programming problem\n",
    "        P = matrix(K)\n",
    "        q = matrix(-np.ones((n_samples, 1)))\n",
    "        \n",
    "        G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
    "        h = matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) / (self.nu * n_samples))))\n",
    "        \n",
    "        A = matrix(1.0, (1, n_samples)) # Equaltiy constraint\n",
    "        b = matrix(1.0)\n",
    "\n",
    "        # Solve quadratic programming problem\n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Extract Lagrange multipliers (alphas)\n",
    "        alphas = np.array(sol['x']).flatten()\n",
    "        support_vector_indices = alphas > 1e-5\n",
    "        self.alphas = alphas[support_vector_indices]\n",
    "        self.support_vectors = X[support_vector_indices]\n",
    "        self.K_sv = K[support_vector_indices][:, support_vector_indices]\n",
    "        \n",
    "        # Compute the intercept term\n",
    "        self.rho = np.mean(self.K_sv @ self.alphas)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # Compute the decision function\n",
    "        K_test = self._kernel_function(X, self.support_vectors)\n",
    "        return np.sum(K_test * self.alphas, axis=1) - self.rho\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict whether the data is an inlier or an outlier\n",
    "        return np.sign(self.decision_function(X))\n",
    "\n",
    "    def plot(self, X):\n",
    "        predictions = self.predict(X)\n",
    "        inliers = X[predictions == 1]\n",
    "        outliers = X[predictions == -1]\n",
    "\n",
    "        # Print IDs of outliers\n",
    "        outlier_ids = np.where(predictions == -1)[0]\n",
    "        print(\"Outlier IDs:\", outlier_ids)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(inliers[:, 0], inliers[:, 1], c='blue', label='Inliers', s=20)\n",
    "        plt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers', s=20)\n",
    "        plt.title('One-Class SVM Results')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7037c01f-5283-4782-9807-aa8c4f817ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationTree:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.is_terminal = False\n",
    "        self.size = 0\n",
    "\n",
    "    def fit(self, X, depth=0):\n",
    "        # Check the termination condition\n",
    "        if depth >= self.max_depth or len(X) <= 1:\n",
    "            self.is_terminal = True\n",
    "            self.size = len(X)\n",
    "            return\n",
    "\n",
    "        # Randomly choose a feature and split value\n",
    "        num_features = X.shape[1]\n",
    "        self.split_feature = np.random.randint(0, num_features)\n",
    "        min_val, max_val = np.min(X[:, self.split_feature]), np.max(X[:, self.split_feature])\n",
    "\n",
    "        if min_val == max_val:  # No meaningful split\n",
    "            self.is_terminal = True\n",
    "            self.size = len(X)\n",
    "            return\n",
    "\n",
    "        self.split_value = np.random.uniform(min_val, max_val)\n",
    "\n",
    "        # Partition the data\n",
    "        left_mask = X[:, self.split_feature] < self.split_value # Mask for left subset\n",
    "        right_mask = ~left_mask # Mask for right subset\n",
    "\n",
    "        self.left = IsolationTree(self.max_depth)\n",
    "        self.right = IsolationTree(self.max_depth)\n",
    "\n",
    "        self.left.fit(X[left_mask], depth + 1)\n",
    "        self.right.fit(X[right_mask], depth + 1)\n",
    "\n",
    "    def path_length(self, X):\n",
    "        path_lengths = np.zeros(X.shape[0])\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            node = self\n",
    "            depth = 0\n",
    "            while not node.is_terminal: # Traverse tree until termianl node reached\n",
    "                if x[node.split_feature] < node.split_value:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "                depth += 1\n",
    "            path_lengths[i] = depth + self._c(node.size)\n",
    "        return path_lengths\n",
    "\n",
    "    @staticmethod\n",
    "    def _c(size):\n",
    "        # Calculate average path length of a binary tree for given size\n",
    "        if size <= 1:\n",
    "            return 0\n",
    "        return 2 * (np.log(size - 1) + 0.5772156649) - (2 * (size - 1) / size)\n",
    "\n",
    "\n",
    "class IsolationForest:\n",
    "    def __init__(self, n_estimators=100, max_samples=256, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.forest = []\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Convert DataFrame to NumPy array if needed\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        num_samples, num_features = X.shape\n",
    "        self.max_samples = min(self.max_samples, num_samples)\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = int(np.ceil(np.log2(self.max_samples)))\n",
    "\n",
    "        self.forest = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            sample_indices = np.random.choice(num_samples, self.max_samples, replace=False)\n",
    "            X_sample = X[sample_indices]\n",
    "            tree = IsolationTree(self.max_depth)\n",
    "            tree.fit(X_sample)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def anomaly_score(self, X):\n",
    "        # Convert DataFrame to NumPy array if needed\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        path_lengths = np.zeros(X.shape[0])\n",
    "        for tree in self.forest:\n",
    "            path_lengths += tree.path_length(X)\n",
    "        path_lengths /= len(self.forest)\n",
    "\n",
    "        # Compute anomaly score based on path lengths\n",
    "        c_n = 2 * (np.log(self.max_samples - 1) + 0.5772156649) - (2 * (self.max_samples - 1) / self.max_samples)\n",
    "        scores = 2 ** (-path_lengths / c_n)\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        scores = self.anomaly_score(X)\n",
    "        return np.where(scores > threshold, -1, 1)  # -1 for anomalies, 1 for normal\n",
    "\n",
    "    def plot(self, X, predictions, feature_names=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        # Assign colors for anomalies and inliers\n",
    "        colors = np.array(['blue' if p == 1 else 'red' for p in predictions])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        if X.shape[1] == 2:  # If data has 2 features\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8)\n",
    "            plt.xlabel(feature_names[0])\n",
    "            plt.ylabel(feature_names[1])\n",
    "        else:  # For more than 2 features, plot only the first two\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8)\n",
    "            plt.xlabel('Feature 1')\n",
    "            plt.ylabel('Feature 2')\n",
    "        plt.title('Isolation Forest - Anomaly Detection')\n",
    "        plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Inliers'),\n",
    "                            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Anomalies')],\n",
    "                   loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d01d7e0-ec41-4ea2-8d9e-3c9ad27f1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify unique ID column\n",
    "def identify_unique_id(dataframe):\n",
    "    continuous_ratio_threshold = 2\n",
    "    unique_id_column = []\n",
    "    for col in dataframe.columns:\n",
    "        if (\n",
    "            dataframe[col].nunique() == len(dataframe)  # Unique values equal to the number of rows\n",
    "            and not pd.api.types.is_float_dtype(dataframe[col])  # Exclude float columns\n",
    "        ):\n",
    "            # Calculate range-to-unique ratio for integer columns\n",
    "            if pd.api.types.is_integer_dtype(dataframe[col]):\n",
    "                col_range = dataframe[col].max() - dataframe[col].min() + 1\n",
    "                if col_range / dataframe[col].nunique() > continuous_ratio_threshold:\n",
    "                    continue  # Likely a continuous integer column, not an ID\n",
    "            unique_id_column = col\n",
    "            print(unique_id_column)\n",
    "    if unique_id_column:\n",
    "        dataframe = dataframe.drop(columns=unique_id_column)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a09a20e-8c87-441e-a487-26af45a5fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_OCSVM(dataframe):\n",
    "    used_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "    print(used_cols)\n",
    "    for i in range(len(used_cols)):\n",
    "        for j in range(i+1, len(used_cols)):\n",
    "            data = dataframe[[used_cols[i], used_cols[j]]]\n",
    "            data_norm = StandardScaler().fit_transform(data)\n",
    "            \n",
    "            # Train custom One-Class SVM\n",
    "            oc_svm = OCSVM(nu=0.01, kernel='rbf', gamma=0.2)\n",
    "            oc_svm.fit(data_norm)\n",
    "            oc_svm.plot(data_norm)\n",
    "            outlier_ids = np.where(oc_svm.predict(data_norm) == -1)[0]\n",
    "            dataframe = dataframe.drop(dataframe.index[outlier_ids]).reset_index(drop=True)\n",
    "            print(len(dataframe))\n",
    "            \n",
    "            # Predict on training data_norm\n",
    "            predictions = oc_svm.predict(data_norm)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1fa83a5-f1c8-4725-bc42-a725ab17f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_DBSCAN(dataframe):\n",
    "    used_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "    print(used_cols)\n",
    "    for i in range(len(used_cols)):\n",
    "        for j in range(i+1, len(used_cols)):\n",
    "            data = dataframe[[used_cols[i], used_cols[j]]]\n",
    "            data_norm = StandardScaler().fit_transform(data)\n",
    "    \n",
    "            # Train custom DBSCAN\n",
    "            dbscan = DBSCAN(eps=4, minPts=5, equation='euclidean')\n",
    "            dbscan.fit(data_norm)\n",
    "            dbscan.plot(data_norm)\n",
    "            outlier_ids = np.where(dbscan.labels == 0)[0]\n",
    "            dataframe = dataframe.drop(dataframe.index[outlier_ids]).reset_index(drop=True)\n",
    "            print(len(dataframe))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b3b686-7fad-474d-8ce1-dd69c1802037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_IF(dataframe):\n",
    "    used_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "    print(used_cols)\n",
    "    for i in range(len(used_cols)):\n",
    "        for j in range(i+1, len(used_cols)):\n",
    "            data = dataframe[[used_cols[i], used_cols[j]]]\n",
    "\n",
    "            # Train Isolation Forest algorithm\n",
    "            iforest = IsolationForest(n_estimators=100, max_samples=len(data), max_depth=100)\n",
    "            iforest.fit(data)\n",
    "            predictions = iforest.predict(data, threshold=0.73)\n",
    "            iforest.plot(data, predictions, feature_names=['Absences', 'GPA'])\n",
    "            \n",
    "            outlier_ids = np.where(predictions == -1)[0]\n",
    "            dataframe = dataframe.drop(dataframe.index[outlier_ids]).reset_index(drop=True)\n",
    "            print(len(dataframe))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87254d6e-2927-4125-8afe-7c8c0a499b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe):\n",
    "    dataframe = identify_unique_id(dataframe)\n",
    "    dataframe = convert_dtype(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65184ce6-db15-4579-a93d-cfff39b6a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtype(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        if is_object_dtype(dataframe[column]):\n",
    "            unique_values = dataframe[column].unique()\n",
    "            value_to_float = {value: float(index) for index, value in enumerate(unique_values)}\n",
    "            dataframe[column] = dataframe[column].map(value_to_float)\n",
    "        if is_string_dtype(dataframe[column]):\n",
    "            unique_values = dataframe[column].unique()\n",
    "            value_to_float = {value: float(index) for index, value in enumerate(unique_values)}\n",
    "            dataframe[column] = dataframe[column].map(value_to_float)\n",
    "    dataframe.info()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d349458-f9b4-4ede-b40e-ea255e5481cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b100e-a559-4919-bd40-888496dc42d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae46e548-0f29-4a30-a194-803d6c6e4089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ee2e8-05b2-4106-879a-4685714a9001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
