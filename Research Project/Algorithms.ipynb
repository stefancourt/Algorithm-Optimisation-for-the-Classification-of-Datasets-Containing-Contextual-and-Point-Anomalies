{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50e0e018-c21f-40b6-acfb-ec57dcb6fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_object_dtype\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from cvxopt import matrix, solvers\n",
    "from sklearn.neighbors import KDTree\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    build_path = os.path.join(os.getcwd(), \"build\", \"Debug\")\n",
    "elif platform.system() == 'Darwin':  # macOS\n",
    "    build_path = os.path.join(os.getcwd(), \"build\")\n",
    "else:\n",
    "    raise EnvironmentError(\"Unsupported operating system\")\n",
    "\n",
    "sys.path.append(build_path)\n",
    " \n",
    "import ocsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d01d7e0-ec41-4ea2-8d9e-3c9ad27f1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify unique ID column\n",
    "def identify_unique_id(dataframe: pd.DataFrame, filename: str) -> pd.DataFrame:\n",
    "    continuous_ratio_threshold = 2\n",
    "    unique_id_column = []\n",
    "    for col in dataframe.columns:\n",
    "        if (\n",
    "            dataframe[col].nunique() == len(dataframe)  # Unique values equal to the number of rows\n",
    "            and not pd.api.types.is_float_dtype(dataframe[col])  # Exclude float columns\n",
    "        ):\n",
    "            # Calculate range-to-unique ratio for integer columns\n",
    "            if pd.api.types.is_integer_dtype(dataframe[col]):\n",
    "                col_range = dataframe[col].max() - dataframe[col].min() + 1\n",
    "                if col_range / dataframe[col].nunique() > continuous_ratio_threshold:\n",
    "                    continue  # Likely a continuous integer column, not an ID\n",
    "            unique_id_column = col\n",
    "    if unique_id_column:\n",
    "        dataframe = dataframe.drop(columns=unique_id_column)\n",
    "    elif filename == 'Datasets/bank-customer-churn-prediction.csv':\n",
    "        dataframe = dataframe.drop(columns=\"customer_id\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65184ce6-db15-4579-a93d-cfff39b6a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtype(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    for column in dataframe.columns:\n",
    "        if is_object_dtype(dataframe[column]):\n",
    "            unique_values = dataframe[column].unique()\n",
    "            value_to_float = {value: float(index) for index, value in enumerate(unique_values)}\n",
    "            dataframe[column] = dataframe[column].map(value_to_float)\n",
    "        if is_string_dtype(dataframe[column]):\n",
    "            unique_values = dataframe[column].unique()\n",
    "            value_to_float = {value: float(index) for index, value in enumerate(unique_values)}\n",
    "            dataframe[column] = dataframe[column].map(value_to_float)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87254d6e-2927-4125-8afe-7c8c0a499b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe: pd.DataFrame, filename: str) -> pd.DataFrame:\n",
    "    dataframe = identify_unique_id(dataframe, filename)\n",
    "    dataframe = convert_dtype(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6bdfe37-7e73-4e03-b2c5-295f5db63a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(dataframe: pd.DataFrame) -> None:\n",
    "    preprocessing(dataframe)\n",
    "    correlation_matrix = dataframe.corr()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f713a932-4506-4f56-b632-0aa3c822be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, eps=1.0, minPts=5) -> None:\n",
    "        self.eps = eps  # Radius for neighbourhood search\n",
    "        self.minPts = minPts  # Minimum number of points to form a cluster\n",
    "\n",
    "    def fit(self, dataframe: np.ndarray) -> List[List[int]]:\n",
    "        # Initialise labels array with -1 (no cluster assigned)\n",
    "        self.labels = np.zeros(len(dataframe), dtype=int) - 1\n",
    "        self.clusters = []  # List to hold all clusters\n",
    "        cidx = 0  # Cluster ID counter\n",
    "\n",
    "        # Build a KDTree for fast nearest-neighbour search\n",
    "        tree = KDTree(dataframe)\n",
    "\n",
    "        for x in range(len(dataframe)):\n",
    "            if self.labels[x] == -1:\n",
    "                neighbours = self.get_neighbours(tree, dataframe, x)\n",
    "                if len(neighbours) < self.minPts:\n",
    "                    self.labels[x] = 0  # Mark point as noise (anomaly)\n",
    "                else:\n",
    "                    cidx += 1\n",
    "                    self.clusters.append([x])\n",
    "                    self.labels[x] = cidx  # Assign cluster ID to the point\n",
    "\n",
    "                    # Expand the cluster with points\n",
    "                    to_visit = neighbours.copy()\n",
    "                    while to_visit:\n",
    "                        y = to_visit.pop()\n",
    "                        if self.labels[y] == -1:  # If point is unvisited\n",
    "                            self.labels[y] = cidx\n",
    "                            self.clusters[cidx - 1].append(y)\n",
    "                            neighbours2 = self.get_neighbours(tree, dataframe, y)\n",
    "                            if len(neighbours2) >= self.minPts:\n",
    "                                to_visit.extend(neighbours2)\n",
    "                        elif self.labels[y] == 0:  # If point is marked as noise\n",
    "                            self.labels[y] = cidx\n",
    "                            self.clusters[cidx - 1].append(y)\n",
    "\n",
    "        return self.clusters\n",
    "\n",
    "    def get_neighbours(self, tree: KDTree, dataframe: np.ndarray, x: int) -> List[int]:\n",
    "        # Use KDTree to find neighbours within eps distance\n",
    "        indices = tree.query_radius([dataframe[x]], r=self.eps)[0]\n",
    "        return indices.tolist()\n",
    "\n",
    "    def plot(self, data: np.ndarray) -> None:\n",
    "        # Create a 2D plot (instead of 3D)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        # Find noise points and clusters\n",
    "        noise_idx = np.where(self.labels == 0)[0]\n",
    "        cluster_idxs = [np.array(c) for c in self.clusters]\n",
    "\n",
    "        # Assign colours to clusters\n",
    "        colours = cm.rainbow(np.linspace(0, 1, len(self.clusters) + 1))\n",
    "\n",
    "        # Plot each cluster\n",
    "        for i, cluster_idx in enumerate(cluster_idxs):\n",
    "            plt.scatter(data[cluster_idx, 0], data[cluster_idx, 1], \n",
    "                        color=colours[i], s=10, label=f'Cluster {i + 1}')\n",
    "\n",
    "        # Plot anomalies\n",
    "        if len(noise_idx) > 0:\n",
    "            plt.scatter(data[noise_idx, 0], data[noise_idx, 1], \n",
    "                        color='black', s=10, label='Anomalies')\n",
    "\n",
    "        # Add labels and legend\n",
    "        plt.title(\"DBSCAN Clustering\")\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff3aa3a6-588f-4c9f-8d65-37026ed9a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocsvm_index_list(model, data: List[Tuple[int, float, float]]) -> List[int]:\n",
    "    inliers_data = []\n",
    "    outliers_data = []\n",
    "    \n",
    "    for point, label in zip(data, [model.predict(point[1:]) for point in data]):\n",
    "        if label == 1:  # Inlier\n",
    "            inliers_data.append(point)\n",
    "        else:  # Outlier\n",
    "            outliers_data.append(point)\n",
    "\n",
    "    index_list = [index[0] for index in outliers_data]\n",
    "\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "526dd657-8e1b-4bea-a7f8-66f17a8f8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocsvm_plot(model, data: List[Tuple[int, float, float]]) -> None:\n",
    "    inliers_data = []\n",
    "    outliers_data = []\n",
    "    \n",
    "    for point, label in zip(data, [model.predict(point[1:]) for point in data]):\n",
    "        if label == 1:  # Inlier\n",
    "            inliers_data.append(point)\n",
    "        else:  # Outlier\n",
    "            outliers_data.append(point)\n",
    "            \n",
    "    # Plot inliers and outliers\n",
    "    inliers_x = [point[1] for point in inliers_data]\n",
    "    inliers_y = [point[2] for point in inliers_data]\n",
    "    outliers_x = [point[1] for point in outliers_data]\n",
    "    outliers_y = [point[2] for point in outliers_data]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(inliers_x, inliers_y, c='blue', label='Inliers', marker='o')\n",
    "    plt.scatter(outliers_x, outliers_y, c='red', label='Outliers', marker='x')\n",
    "    plt.title('Inliers and Outliers')\n",
    "    plt.xlabel('Previous qualification (grade)')\n",
    "    plt.ylabel('Admission grade')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7037c01f-5283-4782-9807-aa8c4f817ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationTree:\n",
    "    def __init__(self, max_depth: int) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.is_terminal = False\n",
    "        self.size = 0\n",
    "\n",
    "    def fit(self, X: np.ndarray, depth: int = 0) -> None:\n",
    "        # Check the termination condition\n",
    "        if depth >= self.max_depth or len(X) <= 1:\n",
    "            self.is_terminal = True\n",
    "            self.size = len(X)\n",
    "            return\n",
    "\n",
    "        # Randomly choose a feature and split value\n",
    "        num_features = X.shape[1]\n",
    "        self.split_feature = np.random.randint(0, num_features)\n",
    "        min_val, max_val = np.min(X[:, self.split_feature]), np.max(X[:, self.split_feature])\n",
    "\n",
    "        if min_val == max_val:  # No meaningful split\n",
    "            self.is_terminal = True\n",
    "            self.size = len(X)\n",
    "            return\n",
    "\n",
    "        self.split_value = np.random.uniform(min_val, max_val)\n",
    "\n",
    "        # Partition the data\n",
    "        left_mask = X[:, self.split_feature] < self.split_value # Mask for left subset\n",
    "        right_mask = ~left_mask # Mask for right subset\n",
    "\n",
    "        self.left = IsolationTree(self.max_depth)\n",
    "        self.right = IsolationTree(self.max_depth)\n",
    "\n",
    "        self.left.fit(X[left_mask], depth + 1)\n",
    "        self.right.fit(X[right_mask], depth + 1)\n",
    "\n",
    "    def path_length(self, X: np.ndarray) -> np.ndarray:\n",
    "        path_lengths = np.zeros(X.shape[0])\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            node = self\n",
    "            depth = 0\n",
    "            while not node.is_terminal: # Traverse tree until termianl node reached\n",
    "                if x[node.split_feature] < node.split_value:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "                depth += 1\n",
    "            path_lengths[i] = depth + self._c(node.size)\n",
    "        return path_lengths\n",
    "\n",
    "    @staticmethod\n",
    "    def _c(size: int) -> float:\n",
    "        # Calculate average path length of a binary tree for given size\n",
    "        if size <= 1:\n",
    "            return 0\n",
    "        return 2 * (np.log(size - 1) + 0.5772156649) - (2 * (size - 1) / size)\n",
    "\n",
    "\n",
    "class IsolationForest:\n",
    "    def __init__(self, n_estimators: int = 100, max_samples: int = 256, max_depth: Optional[int] = None) -> None:\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.forest = []\n",
    "\n",
    "    def fit(self, X: Union[np.ndarray, pd.DataFrame]) -> None:\n",
    "        # Convert DataFrame to NumPy array if needed\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        num_samples, num_features = X.shape\n",
    "        self.max_samples = min(self.max_samples, num_samples)\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = int(np.ceil(np.log2(self.max_samples)))\n",
    "\n",
    "        self.forest = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            sample_indices = np.random.choice(num_samples, self.max_samples, replace=False)\n",
    "            X_sample = X[sample_indices]\n",
    "            tree = IsolationTree(self.max_depth)\n",
    "            tree.fit(X_sample)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def anomaly_score(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:\n",
    "        # Convert DataFrame to NumPy array if needed\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        path_lengths = np.zeros(X.shape[0])\n",
    "        for tree in self.forest:\n",
    "            path_lengths += tree.path_length(X)\n",
    "        path_lengths /= len(self.forest)\n",
    "\n",
    "        # Compute anomaly score based on path lengths\n",
    "        c_n = 2 * (np.log(self.max_samples - 1) + 0.5772156649) - (2 * (self.max_samples - 1) / self.max_samples)\n",
    "        scores = 2 ** (-path_lengths / c_n)\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame], threshold: float = 0.5) -> np.ndarray:\n",
    "        scores = self.anomaly_score(X)\n",
    "        return np.where(scores > threshold, -1, 1)  # -1 for anomalies, 1 for normal\n",
    "\n",
    "    def plot(self, X: Union[np.ndarray, pd.DataFrame], predictions: np.ndarray, feature_names: Optional[List[str]] = None) -> None:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        # Assign colors for anomalies and inliers\n",
    "        colors = np.array(['blue' if p == 1 else 'red' for p in predictions])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        if X.shape[1] == 2:  # If data has 2 features\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8)\n",
    "            plt.xlabel(feature_names[0])\n",
    "            plt.ylabel(feature_names[1])\n",
    "        else:  # For more than 2 features, plot only the first two\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8)\n",
    "            plt.xlabel('Feature 1')\n",
    "            plt.ylabel('Feature 2')\n",
    "        plt.title('Isolation Forest - Anomaly Detection')\n",
    "        plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Inliers'),\n",
    "                            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Anomalies')],\n",
    "                   loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a09a20e-8c87-441e-a487-26af45a5fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_OCSVM(dataframe: pd.DataFrame, filename: str, nu: float, gamma: float) -> pd.DataFrame:\n",
    "    used_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "    print(used_cols)\n",
    "    for i in range(len(used_cols)):\n",
    "        for j in range(i+1, len(used_cols)):\n",
    "            data = [(index, row[used_cols[i]], row[used_cols[j]]) \n",
    "                    for index, row in dataframe.iterrows() \n",
    "                    if not pd.isna(row[used_cols[i]]) and not pd.isna(row[used_cols[j]])]\n",
    "            # Train custom One-Class SVM\n",
    "            oc_svm = ocsvm.OCSVM(nu=nu, kernel=\"rbf\", gamma=0.9)\n",
    "            oc_svm.fit([point[1:] for point in data])\n",
    "            index_list = ocsvm_index_list(oc_svm, data)\n",
    "            dataframe.loc[dataframe.index[index_list], used_cols[i]] = np.nan\n",
    "            dataframe.loc[dataframe.index[index_list], used_cols[j]] = np.nan\n",
    "    dataframe.to_csv(filename, index=False)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1fa83a5-f1c8-4725-bc42-a725ab17f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_DBSCAN(dataframe: pd.DataFrame, filename: str) -> pd.DataFrame:\n",
    "    used_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "    print(used_cols)\n",
    "    for i in range(len(used_cols)):\n",
    "        for j in range(i+1, len(used_cols)):\n",
    "            data = dataframe[[used_cols[i], used_cols[j]]].dropna()\n",
    "            data_norm = StandardScaler().fit_transform(data)\n",
    "    \n",
    "            # Train custom DBSCAN\n",
    "            dbscan = DBSCAN(eps=4, minPts=5)\n",
    "            dbscan.fit(data_norm)\n",
    "            dbscan.plot(data_norm)\n",
    "            outlier_ids = np.where(dbscan.labels == 0)[0]\n",
    "            dataframe.loc[dataframe.index[outlier_ids], used_cols[i]] = np.nan\n",
    "            dataframe.loc[dataframe.index[outlier_ids], used_cols[j]] = np.nan\n",
    "    dataframe.to_csv(filename, index=False)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7b3b686-7fad-474d-8ce1-dd69c1802037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_IF(dataframe: pd.DataFrame, filename: str, threshold: float) -> pd.DataFrame:\n",
    "    used_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "    print(used_cols)\n",
    "    for i in range(len(used_cols)):\n",
    "        for j in range(i+1, len(used_cols)):\n",
    "            data = dataframe[[used_cols[i], used_cols[j]]].dropna()\n",
    "\n",
    "            # Train Isolation Forest algorithm\n",
    "            iforest = IsolationForest(n_estimators=100, max_samples=len(data), max_depth=100)\n",
    "            iforest.fit(data)\n",
    "            predictions = iforest.predict(data, threshold=threshold)\n",
    "            iforest.plot(data, predictions, feature_names=['Absences', 'GPA'])\n",
    "            \n",
    "            outlier_ids = np.where(predictions == -1)[0]\n",
    "            print(outlier_ids)\n",
    "            dataframe.loc[dataframe.index[outlier_ids], used_cols[i]] = np.nan\n",
    "            dataframe.loc[dataframe.index[outlier_ids], used_cols[j]] = np.nan\n",
    "    dataframe.to_csv(filename, index=False)\n",
    "    return dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
